{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis for FD003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from dtaidistance import dtw, clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv('../../TED/CMAPSSData/train_FD003.txt', sep=' ', header = None)\n",
    "test_data_df = pd.read_csv('../../TED/CMAPSSData/test_FD003.txt', sep=' ', header = None)\n",
    "test_labels_at_break_df = pd.read_csv('../../TED/CMAPSSData/RUL_FD003.txt', sep=' ', header = None)\n",
    "\n",
    "train_data_df.drop(train_data_df.columns[[-1, -2]], axis=1, inplace=True)\n",
    "test_data_df.drop(test_data_df.columns[[-1, -2]], axis=1, inplace=True)\n",
    "test_labels_at_break_df.drop(test_labels_at_break_df.columns[[-1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting labels and organising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['ID', 'Cycle', 'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', 'SensorMeasure5', 'SensorMeasure6', 'SensorMeasure7', 'SensorMeasure8', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure11', 'SensorMeasure12', 'SensorMeasure13', 'SensorMeasure14', 'SensorMeasure15', 'SensorMeasure16', 'SensorMeasure17', 'SensorMeasure18', 'SensorMeasure19', 'SensorMeasure20', 'SensorMeasure21']\n",
    "\n",
    "train_data_df.columns = column_names\n",
    "max_cycles_train_df = train_data_df.groupby(['ID'], sort=False)['Cycle'].max().reset_index().rename(columns={'Cycle':'MaxCycleID'})\n",
    "\n",
    "train_data_df = pd.merge(train_data_df, max_cycles_train_df, how='inner', on='ID')\n",
    "train_data_df['RUL'] = train_data_df['MaxCycleID'] - train_data_df['Cycle']\n",
    "train_data_df.drop(columns=['MaxCycleID'], inplace=True)\n",
    "\n",
    "test_data_df.columns = column_names\n",
    "max_cycles_test_df = test_data_df.groupby(['ID'], sort=False)['Cycle'].max().reset_index().rename(columns={'Cycle':'MaxCycleID'})\n",
    "\n",
    "test_labels_at_break_df.columns = ['RUL']\n",
    "test_labels_at_break_df['ID'] = max_cycles_test_df['ID']\n",
    "test_labels_at_break_df['RUL'] = test_labels_at_break_df['RUL'] + max_cycles_test_df['MaxCycleID']\n",
    "\n",
    "test_data_df = pd.merge(test_data_df, test_labels_at_break_df, how='inner', on='ID')\n",
    "test_data_df['RUL'] = test_data_df['RUL'] - test_data_df['Cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = train_data_df['RUL']\n",
    "test_labels_df = test_data_df['RUL']\n",
    "train_labels_at_break_df = train_data_df.groupby('ID').last().reset_index()['RUL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide by engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groupby = train_data_df.groupby(['ID'], sort=False)\n",
    "test_groupby = test_data_df.groupby(['ID'], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs: Data before analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = []\n",
    "for i in range(3):\n",
    "    sensor_names.append('OpSet' + str(i+1))\n",
    "for i in range(21):\n",
    "    sensor_names.append('SensorMeasure' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_by_engine(df_groupby):\n",
    "\n",
    "    groupby_arr = []\n",
    "    for i in range(len(df_groupby)):\n",
    "        groupby_arr.append(df_groupby.get_group(i+1))\n",
    "\n",
    "    for i in range(len(df_groupby)):\n",
    "        groupby_arr[i] = groupby_arr[i].to_numpy()\n",
    "\n",
    "    return groupby_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_engine(train_gb, rows, names, filename):\n",
    "\n",
    "    train_groupby_arr = divide_by_engine(train_gb)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, 4)\n",
    "    flat_axs = axs.flatten()\n",
    "    for sensor in range(len(names)):\n",
    "        flat_axs[sensor].set_title(names[sensor])\n",
    "        for engine in range(len(train_groupby_arr)):\n",
    "            flat_axs[sensor].plot(train_groupby_arr[engine][:, 1], train_groupby_arr[engine][:, sensor + 2])\n",
    "\n",
    "    plt.setp(axs[-1, :], xlabel='Cycles')\n",
    "    plt.setp(axs[:, 0], ylabel='Sensor readings')\n",
    "    fig.set_size_inches(20, 30) \n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_engine(train_groupby, 6, sensor_names, 'fd003-sensors_unprocessed.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we can see which sensors will have a greater impact on the performance of the regressor, and which are irrelevant for this task. I decided to take Sensors: 1, 5, 6, 10, 16, 18, 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking away all non-important sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_used = []\n",
    "sensors_used.append('ID')\n",
    "sensors_used.append('Cycle')\n",
    "for i in range(1, 22):\n",
    "    if i not in [1, 5, 6, 10, 16, 18, 19]:\n",
    "        sensors_used.append(\"SensorMeasure\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df[sensors_used]\n",
    "train_grouppby = train_data_df.groupby(['ID'], sort=False)\n",
    "test_groupby = test_data_df.groupby(['ID'], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.shape\n",
    "ms_used = sensors_used[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "scaled_values = sc.fit_transform(train_data_df.values[:, 2:])\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "pca_values = pca.fit_transform(scaled_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "print(\"Number of components: \" + str(pca.n_components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_, columns = ms_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs = pca.n_components_\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ms_used\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "print(most_important_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_values, x=0, y=1, z=2, color=train_data_df['ID'])\n",
    "fig.show()\n",
    "fig.write_html(\"fd003-PCA-3d_plot.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensors 2, 3, 4, 7, 11, 14, 15, 17, 21 contain 90% of the variance, and therefore will be the most useful sensors to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.to_csv('./fd003-raw_train.csv', sep=' ', columns=train_data_df.columns, index=False)\n",
    "test_data_df[train_data_df.columns].to_csv('./fd003-raw_test.csv', sep=' ', columns=train_data_df.columns, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df.to_csv('./fd003-training_labels.csv', sep=' ', columns=['RUL'], index=False)\n",
    "test_labels_df.to_csv('./fd003-testing_labels.csv', sep=' ', columns=['RUL'], index=False)\n",
    "train_labels_at_break_df.to_csv('./fd003-testing_labels_at_break.csv', sep=' ', columns=['RUL'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "for sensor in ms_used:\n",
    "    train_data_df.loc[:,sensor] = sc.fit_transform(train_data_df[sensor].values.reshape(-1, 1))\n",
    "    test_data_df.loc[:,sensor] = sc.fit_transform(test_data_df[sensor].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groupby = train_data_df.groupby(['ID'], sort=False)\n",
    "plot_by_engine(train_groupby, 6, ms_used, 'fd003-sensors_scaled.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.to_csv('./fd003-scaled_train.csv', sep=' ', columns=train_data_df.columns, index=False)\n",
    "test_data_df[train_data_df.columns].to_csv('./fd003-scaled_test.csv', sep=' ', columns=train_data_df.columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groupby = train_groupby[['ID', 'Cycle'] + ms_used]\n",
    "test_groupby = test_groupby[['ID', 'Cycle'] + ms_used]\n",
    "train_groupby.get_group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_columns = ['ID', 'Cycle'] + ms_used\n",
    "train_groupby_arr = []\n",
    "for i in range(len(train_groupby)):\n",
    "    df = train_groupby.get_group(i+1).copy()\n",
    "    lowess_df = df.copy()\n",
    "    for sensor in range(2, len(smoothing_columns)):\n",
    "        sn = smoothing_columns[sensor]\n",
    "        lowess_df[sn] = pd.DataFrame(lowess(df[sn], np.arange(len(df[sn])), frac=0.25)[:, 1], index=df.index, columns=[sn])\n",
    "    train_groupby_arr.append(lowess_df)\n",
    "\n",
    "tr_joined_lowess_df = train_groupby_arr[0]\n",
    "for i in range(1, len(train_groupby_arr)):\n",
    "    tr_joined_lowess_df = pd.concat([tr_joined_lowess_df, train_groupby_arr[i]])\n",
    "\n",
    "test_groupby_arr = []\n",
    "for i in range(len(test_groupby)):\n",
    "    df = test_groupby.get_group(i+1).copy()\n",
    "    lowess_df = df.copy()\n",
    "    for sensor in range(2, len(smoothing_columns)):\n",
    "        sn = smoothing_columns[sensor]\n",
    "        lowess_df[sn] = pd.DataFrame(lowess(df[sn], np.arange(len(df[sn])), frac=0.25)[:, 1], index=df.index, columns=[sn])\n",
    "    test_groupby_arr.append(lowess_df)\n",
    "\n",
    "test_joined_lowess_df = test_groupby_arr[0]\n",
    "for i in range(1, len(test_groupby_arr)):\n",
    "    test_joined_lowess_df = pd.concat([test_joined_lowess_df, test_groupby_arr[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_joined_lowess_groupby = tr_joined_lowess_df.groupby(['ID'], sort = False)\n",
    "plot_by_engine(tr_joined_lowess_groupby, 6, ms_used, 'fd003-sensors_smoothed.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_joined_lowess_df.to_csv('./fd003-smoothed_train.csv', sep=' ', columns=tr_joined_lowess_df.columns, index=False)\n",
    "test_joined_lowess_df.to_csv('./fd003-smoothed_test.csv', sep=' ', columns=test_joined_lowess_df.columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault modes: Clustering engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_matrices = []\n",
    "for sensor in ms_used:\n",
    "    sensor_matrix = [] \n",
    "    for engine in range(len(train_groupby_arr)):\n",
    "        sensor_matrix.append(train_groupby_arr[engine][sensor].values)\n",
    "    sensor_matrices.append(sensor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = dtw.distance_matrix(sensor_matrices[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ = AgglomerativeClustering().fit(sensor_matrices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clustering.KMedoids(dtw.distance_matrix, {}, k=2)\n",
    "cluster_idx = model.fit(sensor_matrices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_per_engine = []\n",
    "for i in range(len(train_groupby_arr)):\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(train_groupby_arr[i].values.T)\n",
    "    clusters_per_engine.append(kmeans.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
