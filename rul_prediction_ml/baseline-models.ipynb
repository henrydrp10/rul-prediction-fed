{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import LSTM, Convolution1D, GlobalAveragePooling1D, Dense, Dropout, Masking, TimeDistributed\n",
    "import keras_tuner as kt\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.utils import median_survival_times\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import proportional_hazard_test\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "import torchinfo\n",
    "\n",
    "# seed = 35\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed)\n",
    "\n",
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data_analysis/scaled_train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/henry/Documents/Imperial/Masters' Project/rul-prediction-fed/rul_prediction_ml/baseline-models.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/henry/Documents/Imperial/Masters%27%20Project/rul-prediction-fed/rul_prediction_ml/baseline-models.ipynb#ch0000002?line=0'>1</a>\u001b[0m train_data_full_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../../data_analysis/scaled_train_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henry/Documents/Imperial/Masters%27%20Project/rul-prediction-fed/rul_prediction_ml/baseline-models.ipynb#ch0000002?line=1'>2</a>\u001b[0m test_data_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../../data_analysis/scaled_test_data.csv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henry/Documents/Imperial/Masters%27%20Project/rul-prediction-fed/rul_prediction_ml/baseline-models.ipynb#ch0000002?line=3'>4</a>\u001b[0m train_labels_full_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../../data_analysis/training_labels.csv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/tf-m1/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/tf-m1/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.virtualenvs/tf-m1/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.virtualenvs/tf-m1/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.virtualenvs/tf-m1/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m     f,\n\u001b[1;32m   1220\u001b[0m     mode,\n\u001b[1;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1227\u001b[0m )\n\u001b[1;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.virtualenvs/tf-m1/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data_analysis/scaled_train_data.csv'"
     ]
    }
   ],
   "source": [
    "train_data_full_df = pd.read_csv('../../data_analysis/fd002-scaled_train_data.csv', sep=' ')\n",
    "test_data_df = pd.read_csv('../../data_analysis/fd002-scaled_test_data.csv', sep=' ')\n",
    "\n",
    "train_labels_full_df = pd.read_csv('../../data_analysis/fd002-training_labels.csv', sep=' ')\n",
    "test_labels_df = pd.read_csv('../../data_analysis/fd002-testing_labels.csv', sep=' ')\n",
    "test_labels_at_break_df = pd.read_csv('../../TED/CMAPSSData/RUL_FD002.txt', sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaplan Meier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding event_observed column: 0 for not-observed (test_data), 1 for observed (train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_train = train_data_full_df.copy()\n",
    "km_test = test_data_df.copy()\n",
    "km_train['Event Observed'] = 1\n",
    "km_test['Event Observed'] = 0\n",
    "\n",
    "km_total = pd.concat([km_train, km_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations = km_total['Cycle'], event_observed = km_total['Event Observed'])\n",
    "kmf.plot_survival_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_ = kmf.median_survival_time_\n",
    "median_confidence_interval_ = median_survival_times(kmf.confidence_interval_)\n",
    "print(median_)\n",
    "print(median_confidence_interval_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cox Proportional Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_train = train_data_full_df.copy()\n",
    "cox_test = test_data_df.copy()\n",
    "cox_train['Event Observed'] = 1\n",
    "cox_test['Event Observed'] = 0\n",
    "\n",
    "cox_total = pd.concat([cox_train, cox_test])\n",
    "\n",
    "cox_total.drop(columns=['ID', 'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5', 'SensorMeasure18', 'SensorMeasure19'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter()\n",
    "cph.fit(cox_total, duration_col = 'Cycle', event_col = 'Event Observed')\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (10, 6))\n",
    "cph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = train_data_full_df.copy()\n",
    "test_df = test_data_df.copy()\n",
    "train_labels_full_df = train_labels_full_df.copy().clip(upper=125)\n",
    "test_labels_df = test_labels_df.copy()\n",
    "\n",
    "used_sensors = []\n",
    "used_sensors.append(\"ID\")\n",
    "used_sensors.append(\"Cycle\")\n",
    "for i in range(1, 22):\n",
    "    if i not in [1, 5, 6, 10, 16, 18, 19]:\n",
    "        used_sensors.append(\"SensorMeasure\" + str(i))\n",
    "\n",
    "train_full_df = train_full_df[used_sensors]\n",
    "test_df = test_df[used_sensors]\n",
    "\n",
    "# Processed data - Numpy\n",
    "train_full = train_full_df.values\n",
    "test = test_df.values\n",
    "train_labels_full = train_labels_full_df.values.squeeze()\n",
    "test_labels = test_labels_df.values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_train_rul = train_full_df.copy()\n",
    "joined_train_rul['RUL'] = train_labels_full_df['RUL']\n",
    "test_at_break_df = test_df.groupby('ID').last().reset_index()\n",
    "test_at_break = test_at_break_df.values\n",
    "\n",
    "train_labels_at_break = joined_train_rul.groupby('ID').last().reset_index()['RUL'].values\n",
    "test_labels_at_break = test_labels_at_break_df.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groupby_full_df = train_full_df.groupby('ID', sort=False)\n",
    "test_groupby_df = test_df.groupby('ID', sort=False)\n",
    "\n",
    "train_labels_full_df['ID'] = joined_train_rul['ID']\n",
    "train_labels_full_groupby_df = train_labels_full_df.groupby('ID', sort=False)\n",
    "\n",
    "val_indices = np.random.choice(len(train_groupby_full_df), size = int(0.2 * len(train_groupby_full_df)))\n",
    "\n",
    "val_arr = []\n",
    "train_set_arr = []\n",
    "val_labels_arr = []\n",
    "train_set_labels_arr = []\n",
    "\n",
    "for i in range(len(train_groupby_full_df)):\n",
    "    if i in val_indices:\n",
    "        val_arr.append(train_groupby_full_df.get_group(i+1))\n",
    "        val_labels_arr.append(train_labels_full_groupby_df.get_group(i+1)['RUL'])\n",
    "    else:\n",
    "        train_set_arr.append(train_groupby_full_df.get_group(i+1))\n",
    "        train_set_labels_arr.append(train_labels_full_groupby_df.get_group(i+1)['RUL'])\n",
    "\n",
    "val_set_df = val_arr[0]\n",
    "val_labels_df = val_labels_arr[0]\n",
    "for i in range(1, len(val_arr)):\n",
    "    val_set_df = pd.concat([val_set_df, val_arr[i]])\n",
    "    val_labels_df = pd.concat([val_labels_df, val_labels_arr[i]])\n",
    "\n",
    "train_set_df = train_set_arr[0]\n",
    "train_set_labels_df = train_set_labels_arr[0]\n",
    "for i in range(1, len(train_set_arr)):\n",
    "    train_set_df = pd.concat([train_set_df, train_set_arr[i]])\n",
    "    train_set_labels_df = pd.concat([train_set_labels_df, train_set_labels_arr[i]])\n",
    "\n",
    "train_set = train_set_df.values\n",
    "train_set_labels = train_set_labels_df.values\n",
    "val_set = val_set_df.values\n",
    "val_labels = val_labels_df.values\n",
    "val_labels = np.expand_dims(val_labels, axis = 1)\n",
    "train_set_labels = np.expand_dims(train_set_labels, axis = 1)\n",
    "train_labels_full = np.expand_dims(train_labels_full, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_full_df.shape, train_full.shape)\n",
    "print(train_labels_full_df.shape, train_labels_full.shape)\n",
    "print(train_set_df.shape, train_set.shape)\n",
    "print(train_set_labels_df.shape, train_set_labels.shape)\n",
    "print(val_set_df.shape, val_set.shape)\n",
    "print(val_labels_df.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(data_df, labels_df, window_length, mode = 'train'):\n",
    "\n",
    "    if mode == 'train':\n",
    "\n",
    "        labels_df['ID'] = data_df['ID']\n",
    "\n",
    "        data_groupby = data_df.groupby('ID', sort=False)\n",
    "        labels_groupby = labels_df.groupby('ID', sort=False)\n",
    "\n",
    "        val_indices = np.random.choice(len(data_groupby), size = int(0.2 * len(data_groupby)))\n",
    "\n",
    "        tr_data_eng_arr = []\n",
    "        tr_labels_eng_arr = []\n",
    "\n",
    "        val_data_eng_arr = []\n",
    "        val_labels_eng_arr = []\n",
    "\n",
    "        for i in range(len(data_groupby)):\n",
    "            if i in val_indices:\n",
    "                val_data_eng_arr.append(data_groupby.get_group(i+1))\n",
    "            else:\n",
    "                tr_data_eng_arr.append(data_groupby.get_group(i+1))\n",
    "\n",
    "        for i in range(len(labels_groupby)):\n",
    "            if i in val_indices:\n",
    "                val_labels_eng_arr.append(labels_groupby.get_group(i+1))\n",
    "            else:\n",
    "                tr_labels_eng_arr.append(labels_groupby.get_group(i+1))\n",
    "\n",
    "        tr_data_windows = []\n",
    "        tr_label_windows = []\n",
    "        for index in range(len(tr_data_eng_arr)):\n",
    "            tr_data_arr = tr_data_eng_arr[index].to_numpy()\n",
    "            tr_labels_arr = tr_labels_eng_arr[index].to_numpy()\n",
    "            for t in range(tr_data_arr.shape[0] - window_length + 1):\n",
    "                tr_data_windows.append(tr_data_arr[t:t+window_length, :])\n",
    "                tr_label_windows.append(tr_labels_arr[t+window_length - 1, 0])\n",
    "\n",
    "        val_data_windows = []\n",
    "        val_label_windows = []\n",
    "        for index in range(len(val_data_eng_arr)):\n",
    "            val_data_arr = val_data_eng_arr[index].to_numpy()\n",
    "            val_labels_arr = val_labels_eng_arr[index].to_numpy()\n",
    "            for t in range(val_data_arr.shape[0] - window_length + 1):\n",
    "                val_data_windows.append(val_data_arr[t:t+window_length, :])\n",
    "                val_label_windows.append(val_labels_arr[t+window_length - 1, 0])\n",
    "\n",
    "        return np.array(tr_data_windows), np.array(tr_label_windows), np.array(val_data_windows), np.array(val_label_windows)\n",
    "\n",
    "    else:\n",
    "\n",
    "        labels_df['ID'] = data_df['ID']\n",
    "\n",
    "        data_groupby = data_df.groupby('ID', sort=False)\n",
    "        labels_groupby = labels_df.groupby('ID', sort=False)\n",
    "        data_eng_arr = []\n",
    "        labels_eng_arr = []\n",
    "\n",
    "        for i in range(len(data_groupby)):\n",
    "            data_eng_arr.append(data_groupby.get_group(i+1))\n",
    "\n",
    "        for i in range(len(labels_groupby)):\n",
    "            labels_eng_arr.append(labels_groupby.get_group(i+1))\n",
    "\n",
    "        data_windows = []\n",
    "        label_windows = []\n",
    "        for index in range(len(data_eng_arr)):\n",
    "            data_arr = data_eng_arr[index].to_numpy()\n",
    "            labels_arr = labels_eng_arr[index].to_numpy()\n",
    "            data_windows.append(data_arr[-window_length:, :])\n",
    "            label_windows.append(labels_arr[-1, 0])\n",
    "\n",
    "        return np.array(data_windows), np.array(label_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'bootstrap': [True, False], \n",
    "    'max_depth': [6, 7, 8, 9, 10], \n",
    "    'min_samples_leaf': [30, 35, 40, 45, 50],\n",
    "    'max_features': ['auto', 'log2', 'sqrt'], \n",
    "    'n_estimators': [100 * x for x in range(5, 11)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "rand_search = RandomizedSearchCV(estimator = rf, param_distributions = rf_param_grid, cv = 3, n_jobs = 1, verbose = 3, return_train_score=True)\n",
    "rand_search.fit(train_full, train_labels_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = rand_search.predict(test_at_break).round()\n",
    "print(rand_search.best_params_)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels_at_break, predictions_rf))\n",
    "print(\"RMSE: \" + str(rmse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(train_full, train_labels_full)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gb = gb.predict(test_at_break).round()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test_labels_at_break, predictions_gb))\n",
    "print(\"RMSE: \" + str(rmse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmr = SVR(C=1.0, epsilon=0.2)\n",
    "svmr.fit(train_full, train_labels_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svmr = svmr.predict(test_at_break).round()\n",
    "rmse = np.sqrt(mean_squared_error(test_labels_at_break, predictions_svmr))\n",
    "print(\"RMSE: \" + str(rmse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(32, activation = 'relu', input_dim = train_set.shape[1]))\n",
    "mlp_model.add(Dense(64, activation = 'relu'))\n",
    "mlp_model.add(Dense(128 , activation = 'relu'))\n",
    "mlp_model.add(Dropout(0.1))\n",
    "mlp_model.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "\n",
    "mlp_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "mlp_model.summary()\n",
    "mlp_model.save_weights('mlp_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "mlp_model.load_weights('mlp_weights.h5') \n",
    "mlp_history = mlp_model.fit(train_set, train_set_labels, epochs=100, validation_data = (val_set, val_labels), batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAIN AND VALIDATION LOSS\n",
    "def plot_loss(fit_history):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    plt.plot(range(1, len(fit_history.history['loss'])+1), fit_history.history['loss'], label='train')\n",
    "    plt.plot(range(1, len(fit_history.history['val_loss'])+1), fit_history.history['val_loss'], label='validate')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(mlp_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FUNCTION\n",
    "def evaluate(actual, pred, mode = 'test'):\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(actual, pred)\n",
    "    print(mode + ' set RMSE: ' + str(rmse) + ', R2: ' + str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "train_full_pred = mlp_model.predict(train_full)\n",
    "evaluate(train_labels_full, train_full_pred, 'train')\n",
    "\n",
    "test_at_break_pred = mlp_model.predict(test_at_break)\n",
    "evaluate(test_labels_at_break, test_at_break_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 20\n",
    "cnn_tr_data, cnn_tr_labels, cnn_val_data, cnn_val_labels = get_windows(train_full_df, train_labels_full_df, window_length, mode='train')\n",
    "cnn_test_data, cnn_test_labels = get_windows(test_df, test_labels_df, 20, mode = 'test')\n",
    "\n",
    "cnn_tr_labels = np.expand_dims(cnn_tr_labels, axis=1)\n",
    "cnn_val_labels = np.expand_dims(cnn_val_labels, axis=1)\n",
    "cnn_test_labels = np.expand_dims(cnn_test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Convolution1D(256, 3, input_shape = (window_length, cnn_tr_data.shape[2])))\n",
    "cnn_model.add(Convolution1D(128, 3, activation = 'relu'))\n",
    "cnn_model.add(Convolution1D(64, 3, activation = 'relu'))\n",
    "cnn_model.add(GlobalAveragePooling1D(data_format = 'channels_last', keepdims = False))\n",
    "cnn_model.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "cnn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "cnn_model.save_weights('simple_lstm_weights.h5')\n",
    "\n",
    "cnn_model.compile(loss='mean_squared_error', optimizer='adam')  \n",
    "cnn_model.load_weights('simple_lstm_weights.h5')  \n",
    "\n",
    "history = cnn_model.fit(cnn_tr_data, cnn_tr_labels,\n",
    "                        validation_data=(cnn_val_data, cnn_val_labels),\n",
    "                        epochs=50,\n",
    "                        batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FUNCTION\n",
    "def evaluate(actual, pred, mode = 'test'):\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(actual, pred)\n",
    "    print(mode + ' set RMSE: ' + str(rmse) + ', R2: ' + str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "train_cnn_pred = cnn_model.predict(cnn_tr_data)\n",
    "evaluate(cnn_tr_labels, train_cnn_pred, 'train')\n",
    "\n",
    "test_cnn_pred = cnn_model.predict(cnn_test_data)\n",
    "evaluate(cnn_test_labels, test_cnn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(32, activation='tanh', input_shape=(window_length, cnn_tr_data.shape[2])))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "lstm_model.save_weights('simple_lstm_weights.h5')\n",
    "\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam')  \n",
    "lstm_model.load_weights('simple_lstm_weights.h5')  \n",
    "\n",
    "history = lstm_model.fit(cnn_tr_data, cnn_tr_labels,\n",
    "                        validation_data=(cnn_val_data, cnn_val_labels),\n",
    "                        epochs=50,\n",
    "                        batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT LOSS HISTORY\n",
    "def plot_loss(fit_history):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    plt.plot(range(1, len(fit_history.history['loss'])+1), fit_history.history['loss'], label='train')\n",
    "    plt.plot(range(1, len(fit_history.history['val_loss'])+1), fit_history.history['val_loss'], label='validate')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FUNCTION\n",
    "def evaluate(actual, pred, mode = 'test'):\n",
    "    mse = mean_squared_error(actual, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    variance = r2_score(actual, pred)\n",
    "    print(mode + ' set RMSE: ' + str(rmse) + ', R2: ' + str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "train_cnn_pred = lstm_model.predict(cnn_tr_data)\n",
    "evaluate(cnn_tr_labels, train_cnn_pred, 'train')\n",
    "\n",
    "test_cnn_pred = lstm_model.predict(cnn_test_data)\n",
    "evaluate(cnn_test_labels, test_cnn_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7db919a617d352c2dafc671a388d65b97a697f0535deb448e14c82fa78835651"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
