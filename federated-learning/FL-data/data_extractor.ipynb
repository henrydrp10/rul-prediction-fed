{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract FL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data_analysis/fd003/fd003-scaled_train.csv', sep=' ')\n",
    "test_df = pd.read_csv('../../data_analysis/fd003/fd003-scaled_test.csv', sep=' ')\n",
    "train_labels_df = pd.read_csv('../../data_analysis/fd003/fd003-training_labels.csv', sep=' ')\n",
    "test_labels_df = pd.read_csv('../../data_analysis/fd003/fd003-testing_labels.csv', sep=' ')\n",
    "test_labels_at_break_df = pd.read_csv('../../TED/CMAPSSData/RUL_FD003.txt', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_df.columns\n",
    "ms_used = []\n",
    "for i in range(1, 22):\n",
    "    if i not in [1, 5, 6, 9, 10, 14, 16, 18, 19]:\n",
    "        ms_used.append('SensorMeasure' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_at_break_df = test_df.groupby(['ID']).last().reset_index()\n",
    "test_labels_at_break_df.columns = ['RUL', 'NaN']\n",
    "test_labels_at_break_df.drop(columns = ['NaN'], inplace = True)\n",
    "train_labels_df[ms_used] = train_df[ms_used]\n",
    "test_labels_at_break_df[ms_used] = test_at_break_df[ms_used]\n",
    "train_labels_df['ID'] = train_df['ID']\n",
    "test_labels_at_break_df['ID'] = test_at_break_df['ID']\n",
    "train_df = train_labels_df.copy()\n",
    "test_df = test_labels_at_break_df.copy()\n",
    "train_df['RUL'] = train_labels_df['RUL'].clip(upper=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_at_break_df['RUL'] = test_labels_at_break_df['RUL']\n",
    "test_at_break_df.to_csv('./tf/fd003/scaled/test_partition.csv', sep = ',', index = False)\n",
    "\n",
    "test_at_break_df = test_at_break_df.reindex(columns=['ID', 'Cycle', 'RUL'] + ms_used)\n",
    "test_at_break_df.drop(columns = ['ID', 'Cycle'], inplace = True)\n",
    "new_cols = ['y'] + ['x' + str(num) for num in range(len(ms_used))]\n",
    "test_at_break_df.columns = new_cols\n",
    "\n",
    "test_at_break_df.to_csv('./decision-trees/fd003/scaled/test_partition.csv', sep = ',', index_label = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gb = train_df.groupby(['ID'], sort = False)\n",
    "test_gb = test_df.groupby(['ID'], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n",
      "[0.6, 0.4]\n",
      "[0.7, 0.3]\n",
      "[0.8, 0.2]\n",
      "[0.9, 0.1]\n",
      "[0.4, 0.3, 0.3]\n",
      "[0.4, 0.4, 0.2]\n",
      "[0.5, 0.4, 0.1]\n",
      "[0.6, 0.3, 0.1]\n",
      "[0.7, 0.2, 0.1]\n",
      "[0.8, 0.1, 0.1]\n",
      "[0.3, 0.3, 0.2, 0.2]\n",
      "[0.3, 0.3, 0.3, 0.1]\n",
      "[0.4, 0.3, 0.2, 0.1]\n",
      "[0.4, 0.4, 0.1, 0.1]\n",
      "[0.5, 0.3, 0.1, 0.1]\n",
      "[0.6, 0.2, 0.1, 0.1]\n",
      "[0.7, 0.1, 0.1, 0.1]\n",
      "[0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "[0.3, 0.2, 0.2, 0.2, 0.1]\n",
      "[0.3, 0.3, 0.2, 0.1, 0.1]\n",
      "[0.4, 0.3, 0.1, 0.1, 0.1]\n",
      "[0.5, 0.2, 0.1, 0.1, 0.1]\n",
      "[0.6, 0.1, 0.1, 0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# Args: List of pcts -> ex: [0.75, 0.2, 0.05]\n",
    "def split_data(pcts, mode = 'train'):\n",
    "\n",
    "    gb = train_gb if mode == 'train' else test_gb\n",
    "\n",
    "    idx_remaining = [i for i in range(1, len(gb) + 1)]\n",
    "    nums = []\n",
    "    indices_concat = []\n",
    "    for pct in pcts:\n",
    "        nums.append(int(pct * len(gb)))\n",
    "    n_workers = len(pcts)\n",
    "\n",
    "    for i in range(n_workers):\n",
    "        idx_worker = np.sort(np.random.choice(idx_remaining, nums[i], replace = False))\n",
    "        indices_concat.append(idx_worker) \n",
    "        idx_remaining = np.setdiff1d(idx_remaining, idx_worker)\n",
    "\n",
    "    for i in range(len(indices_concat)):\n",
    "        id_list = indices_concat[i]\n",
    "        df = gb.get_group(id_list[0])\n",
    "        for j in range(1, len(id_list)):\n",
    "            df = pd.concat([df, gb.get_group(id_list[j])])\n",
    "        folder_name = str('')\n",
    "        for pct in pcts:\n",
    "            folder_name += str(int(pct * 100))\n",
    "            folder_name += '-'\n",
    "        folder_name = folder_name[:-1]\n",
    "        # df.to_csv('./tf/fd003/scaled/' + str(n_workers) + ' workers/' + folder_name + '/' + mode + '_partition_' + str(i) + '.csv', sep=',', index = False)\n",
    "        new_cols = ['y'] + ['x' + str(num) for num in range(len(ms_used))]\n",
    "        df.columns = new_cols + ['ID']\n",
    "        df[new_cols].to_csv('./decision-trees/fd003/scaled/' + str(n_workers) + ' workers/' + folder_name + '/' + mode + '_partition_' + str(i) + '.csv', sep=',', index_label='id')\n",
    "\n",
    "pct_list_total = [\n",
    "    [0.5, 0.5], \n",
    "    [0.6, 0.4], \n",
    "    [0.7, 0.3], \n",
    "    [0.8, 0.2], \n",
    "    [0.9, 0.1]\n",
    "    ]\n",
    "pct_list_total.extend([\n",
    "    [0.4, 0.3, 0.3], \n",
    "    [0.4, 0.4, 0.2], \n",
    "    [0.5, 0.4, 0.1], \n",
    "    [0.6, 0.3, 0.1], \n",
    "    [0.7, 0.2, 0.1], \n",
    "    [0.8, 0.1, 0.1]\n",
    "    ])\n",
    "pct_list_total.extend([\n",
    "    [0.3, 0.3, 0.2, 0.2], \n",
    "    [0.3, 0.3, 0.3, 0.1],  \n",
    "    [0.4, 0.3, 0.2, 0.1], \n",
    "    [0.4, 0.4, 0.1, 0.1], \n",
    "    [0.5, 0.3, 0.1, 0.1],\n",
    "    [0.6, 0.2, 0.1, 0.1],\n",
    "    [0.7, 0.1, 0.1, 0.1]\n",
    "    ])\n",
    "\n",
    "pct_list_total.extend([\n",
    "    [0.2, 0.2, 0.2, 0.2, 0.2], \n",
    "    [0.3, 0.2, 0.2, 0.2, 0.1], \n",
    "    [0.3, 0.3, 0.2, 0.1, 0.1], \n",
    "    [0.4, 0.3, 0.1, 0.1, 0.1], \n",
    "    [0.5, 0.2, 0.1, 0.1, 0.1], \n",
    "    [0.6, 0.1, 0.1, 0.1, 0.1]\n",
    "    ])\n",
    "\n",
    "for pct_list in pct_list_total:\n",
    "    print(pct_list)\n",
    "    split_data(pct_list, mode = 'train')\n",
    "    split_data(pct_list, mode = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('flower-3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb86fb12fd2a818717064780a7aa0239eba96379dea6310fea96d8b7623a3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
